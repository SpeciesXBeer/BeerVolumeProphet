
#Below, there are 3 values for each prior scale adjustment, and the code will run cross validation on the a1s1 final Prophet.  3 x 3 x 3 = 27 total runs.  
#This can take a long time to run, so choose your hyper-parameter tuning wisely.

### Analyze best hyperparameter tuning for the a1s1 Meta Prophet
a1s1_param_grid = {  
    'changepoint_prior_scale': [0.01, 0.1, 1.6],
    'seasonality_prior_scale': [0.01, 0.1, 1.0],
    'changepoint_range': [0.75, 0.8, 0.95],
}
# Generate all combinations of parameters, for a1s1 Prophet
a1s1_all_params = [dict(zip(a1s1_param_grid.keys(), 
                                a1s1)) for a1s1 in itertools.product(*a1s1_param_grid.values())]
a1s1_mapes = []  # Store the RMSEs for each params here
# Use cross validation to evaluate all Agency 1 and SKU 1 parameters 
#Remove/swap hastags below to cross evaluate numerous variables in
#changepoint, seasonality, and range in param grid above.
for a1s1params in a1s1_all_params:
    a1s1_prophet = Prophet(**a1s1params).fit(a1s1_prophet_feed) 
    # Fit model with given params
    a1s1_crossval = cross_validation(a1s1_prophet, period='31 days', horizon = '31 days')
    a1s1_performance = performance_metrics(a1s1_crossval, rolling_window=1)
    a1s1_mapes.append(a1s1_performance['mape'].values[0])
a1s1_tuning_results = pd.DataFrame(a1s1_all_params)
a1s1_tuning_results['mape'] = a1s1_mapes
best_a1s1_params = a1s1_all_params[np.argmin(a1s1_mapes)]
print(best_a1s1_params)
